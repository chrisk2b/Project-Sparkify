{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project\n",
    "This Notebook contains the code of the Data Science Nanodegree Sparkify Capstone Project. Sparkify is a fictional music provider just like Spotify. In the Project, we analyse event data of Sparkify users. The data is given in terms of an event log which records events such as to which songs users are listening, are they adding friends or are they downgrading from paid to a free account etc.\n",
    "We work only on a tiny subset (128MB) of the full dataset available (12GB) since the analysis is done locally. The goal is to predict churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \\\n",
    "    IDF, StringIndexer, VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Session\n",
    "We set up a spark session where the master node is given by the local computer. In a real spark cluster, we would refer to the IP address of the machine which defines the master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Capstone_Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Data\n",
    "First, we load the data into a Spark Data Frame, the data is contained in a JSON file in the data folder of the project with the name ‘mini_sparkify_event_data.json’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = r\"C:\\Repositories\\Sparkify-Project\\data\\mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.json(path_to_data)\n",
    "data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get an idea of the columns and the corresponding data types, we print out the schema of the data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset\n",
    "In this section, we will clean the dataset, i.e. checking for invalid or missing data - for example, records without userids or sessionids. Since we will primarily with the SQL abstraction of spark data frames, we need to register a temporary view of the data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check if there are missing values (null values) in the userId column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(userId)|\n",
      "+-------------+\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE userId IS NULL\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing user IDs, but let’s also check if there are IDs which contain an empty string (which could correspond to no registered users): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(userId)|\n",
      "+-------------+\n",
      "|         8346|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE userId == ''\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are empy user IDs. Let's check for invalid session IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(userId)|\n",
      "+-------------+\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        sessionId == ''\n",
    "    OR\n",
    "        sessionId IS NULL\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no invalid session IDs. We will now remove the invalid user IDs from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        userId != ''\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "In this section, we perform EDA to get a better understanding of the data. Further, we need to provide a formal definition of churn. We start with the investigation of the ‘page’ column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>count(UserId)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancel</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Submit Downgrade</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thumbs Down</td>\n",
       "      <td>2546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home</td>\n",
       "      <td>10082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downgrade</td>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Roll Advert</td>\n",
       "      <td>3933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logout</td>\n",
       "      <td>3226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Save Settings</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cancellation Confirmation</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>About</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Settings</td>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Add to Playlist</td>\n",
       "      <td>6526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Add Friend</td>\n",
       "      <td>4277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NextSong</td>\n",
       "      <td>228108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Thumbs Up</td>\n",
       "      <td>12551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Help</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Upgrade</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Error</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Submit Upgrade</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         page  count(UserId)\n",
       "0                      Cancel             52\n",
       "1            Submit Downgrade             63\n",
       "2                 Thumbs Down           2546\n",
       "3                        Home          10082\n",
       "4                   Downgrade           2055\n",
       "5                 Roll Advert           3933\n",
       "6                      Logout           3226\n",
       "7               Save Settings            310\n",
       "8   Cancellation Confirmation             52\n",
       "9                       About            495\n",
       "10                   Settings           1514\n",
       "11            Add to Playlist           6526\n",
       "12                 Add Friend           4277\n",
       "13                   NextSong         228108\n",
       "14                  Thumbs Up          12551\n",
       "15                       Help           1454\n",
       "16                    Upgrade            499\n",
       "17                      Error            252\n",
       "18             Submit Upgrade            159"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        page,\n",
    "        COUNT(UserId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    GROUP BY\n",
    "        page\n",
    "    \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us already a good indicator for the churn definition: we could define churned users as those users who have visited the ‘Cancelation Confirmation’ page. Further, we could take the ‘Submit Downgrade’ column into account. But maybe it is better to use it as a feature for the churn prediction, since it could be considered as an early warning indicator for churn, if a user submits a downgrade. We will investigate that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with the definition of an indicator if a user has visited the ‘Cancel Confirmation’ page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        CASE\n",
    "            WHEN page == 'Cancellation Confirmation' THEN 1\n",
    "            ELSE 0\n",
    "        END as hasVisitCancel\n",
    "    FROM\n",
    "        data_tbl\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on that indicator, we can identify all churned users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churned_users = spark.sql(\"\"\"\n",
    "                            SELECT\n",
    "                                DISTINCT userID\n",
    "                            FROM\n",
    "                                data_tbl\n",
    "                            WHERE\n",
    "                                hasVisitCancel = 1\n",
    "                            \"\"\").toPandas().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churned_users = [item[0] for item in churned_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['125', '51', '54', '100014', '101']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churned_users[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us define a column churn by using a user defined function which flags all churned user with a 1 and 0 oterwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_churned = lambda user: 1 if user in churned_users else 0\n",
    "spark.udf.register('has_churned', has_churned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        has_churned(userId) as churn\n",
    "    FROM\n",
    "        data_tbl\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many churned and non-churned user we have in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|churn|count(DISTINCT userId)|\n",
      "+-----+----------------------+\n",
      "|    0|                   173|\n",
      "|    1|                    52|\n",
      "+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT\n",
    "              churn,\n",
    "              count(distinct userId)\n",
    "            FROM\n",
    "                data_tbl\n",
    "            GROUP BY\n",
    "                churn\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates some imbalance between churned and non-churned users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "In this section we are looking for features which provide signals if a user will potentially churn. As mentioned above, an early warning indicator could be the fact if a user has visited the ‘Submit Downgrade’ page. Let us first investigate if this assumption holds true. We apply analog steps as in the definition of the 'churn' column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_with_downgrade = spark.sql(\"select distinct userId from data_tbl where page = 'Submit Downgrade'\")\n",
    "user_with_downgrade = user_with_downgrade.toPandas()['userId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_sub_downgrade = udf(lambda user: 1 if user in user_with_downgrade else 0, IntegerType())\n",
    "spark.udf.register('has_sub_downgrade', has_sub_downgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        has_sub_downgrade(userId) as hasSubDowngrade\n",
    "    FROM\n",
    "        data_tbl\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_downgrade = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        hasSubDowngrade,\n",
    "        churn\n",
    "    FROM\n",
    "        data_tbl\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_downgrade.createOrReplaceTempView('feature_downgrade_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------+\n",
      "|churn|(CAST(sum(CAST(hasSubDowngrade AS BIGINT)) AS DOUBLE) / CAST(count(DISTINCT userId) AS DOUBLE))|\n",
      "+-----+-----------------------------------------------------------------------------------------------+\n",
      "|    0|                                                                            0.23121387283236994|\n",
      "|    1|                                                                            0.17307692307692307|\n",
      "+-----+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select churn, sum(hasSubDowngrade)/count(distinct userId) from feature_downgrade_tbl group by churn\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is basically no (major) difference in submitting a downgrade for churned and non-churned users. Interestingly, non-churned users are submitting downgrades more frequently. As a result, we plan to not to include the submit downgrade indicator as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Usage Time\n",
    "We can use the length column to define a 'usage time' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_usage_time = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        sum(nvl(length, 0)) as usageTime\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'NextSong'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "feature_usage_time.createOrReplaceTempView('feature_usage_time_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usage_time = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        churn,\n",
    "        sum(length)/count(distinct userId) as usageTime\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'NextSong'\n",
    "    GROUP BY\n",
    "        churn\n",
    "    \"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn</th>\n",
       "      <th>usageTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>276166.937468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>174014.268551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  churn      usageTime\n",
       "0     0  276166.937468\n",
       "1     1  174014.268551"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that churned usere have a lower usage time. Hence we will include the usage time as feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_usage_time = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        sum(nvl(length, 0)) as usageTime\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'NextSong'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "feature_usage_time.createOrReplaceTempView('feature_usage_time_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Number of Added Friends\n",
    "We assume that non-churned users will add friends more frequently, lets check if this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------+\n",
      "|churn|(CAST(count(1) AS DOUBLE) / CAST(count(DISTINCT userId) AS DOUBLE))|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "|    0|                                                  22.47530864197531|\n",
      "|    1|                                                 14.454545454545455|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        churn,\n",
    "        count(1)/count(DISTINCT userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'Add Friend'\n",
    "    GROUP BY\n",
    "        churn\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that non-churned users indeed add friends more often. Hence we include that as a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_add_friends = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        count(1) as n_addFriend\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'Add Friend'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_add_friends.createOrReplaceTempView('feature_add_friends_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Asking for Help\n",
    "It seems to be plausible that non-churned usere contact the help more frequently. Lets see if this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------+\n",
      "|churn|(CAST(count(1) AS DOUBLE) / CAST(count(DISTINCT userId) AS DOUBLE))|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "|    0|                                                  8.154362416107382|\n",
      "|    1|                                                  5.558139534883721|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        churn,\n",
    "        count(1)/count(DISTINCT userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'Help'\n",
    "    GROUP BY\n",
    "        churn\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, non-churned users contact the help page more often. Thus, we include it as feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_help = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        count(1) as n_help\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        page = 'Help'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_help.createOrReplaceTempView('feature_help_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Length Paid User\n",
    "Maybe the time of using payed services is longer for non-churned users. Lets investigate:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------+\n",
      "|churn|(CAST(count(1) AS DOUBLE) / CAST(count(DISTINCT userId) AS DOUBLE))|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "|    0|                                                 1472.5348837209303|\n",
      "|    1|                                                  902.1111111111111|\n",
      "+-----+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        churn,\n",
    "        count(1)/count(DISTINCT userId)\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        level = 'paid'\n",
    "    GROUP BY\n",
    "        churn\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our assumption is correct, hence we include the corresponding feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length_paid = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        sum(nvl(length, 0)) as lengthAsPaid\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        level = 'paid'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_length_paid.createOrReplaceTempView('feature_length_paid_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Length Free User\n",
    "Analog as above, we define a feature which measures the time of being a user which uses free services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length_free = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT userId,\n",
    "        sum(nvl(length, 0)) as lengthAsFree\n",
    "    FROM\n",
    "        data_tbl\n",
    "    WHERE\n",
    "        level = 'free'\n",
    "    GROUP BY\n",
    "        userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_length_free.createOrReplaceTempView('feature_length_free_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can combine all the features, we need to do some minimal cleaning, since the filtering in the feature definition can lead to the loss of some user IDs. But since we want to include all users, we need to left join some of the feature tables with the overall data table in order to not lose some of the users:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_add_friends = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT d.userId,\n",
    "        n_addFriend\n",
    "    FROM\n",
    "        data_tbl as d\n",
    "    LEFT JOIN\n",
    "        feature_add_friends_tbl as f\n",
    "    ON\n",
    "        d.userId=f.userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_add_friends.createOrReplaceTempView('feature_add_friends_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_help = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT d.userId,\n",
    "        n_help\n",
    "    FROM\n",
    "        data_tbl as d\n",
    "    LEFT JOIN\n",
    "        feature_help_tbl as f\n",
    "    ON\n",
    "        d.userId=f.userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_help.createOrReplaceTempView('feature_help_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length_paid = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT d.userId,\n",
    "        lengthAsPaid\n",
    "    FROM\n",
    "        data_tbl as d\n",
    "    LEFT JOIN\n",
    "        feature_length_paid_tbl as f\n",
    "    ON\n",
    "        d.userId=f.userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_length_paid.createOrReplaceTempView('feature_length_paid_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_length_free = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT d.userId,\n",
    "        f.lengthAsFree\n",
    "    FROM\n",
    "        data_tbl as d\n",
    "    LEFT JOIN\n",
    "        feature_length_free_tbl as f\n",
    "    ON\n",
    "        d.userId=f.userId\n",
    "    \"\"\")\n",
    "\n",
    "feature_length_free.createOrReplaceTempView('feature_length_free_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to include all the features in one table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        usage.userId,\n",
    "        free.lengthAsFree,\n",
    "        paid.lengthAsPaid,\n",
    "        n_help,\n",
    "        n_addFriend,\n",
    "        usageTime \n",
    "    FROM\n",
    "        feature_usage_time_tbl as usage\n",
    "    LEFT JOIN\n",
    "        feature_length_free_tbl as free\n",
    "    ON\n",
    "        usage.userId=free.userId \n",
    "    LEFT JOIN\n",
    "        feature_length_paid_tbl as paid\n",
    "    ON\n",
    "        paid.userId = free.userId \n",
    "    LEFT JOIN\n",
    "        feature_help_tbl as help\n",
    "    ON\n",
    "        help.userId=paid.userId \n",
    "    LEFT JOIN\n",
    "        feature_add_friends_tbl as friends\n",
    "    ON\n",
    "        friends.userId=help.userId\n",
    "    \"\"\")\n",
    "\n",
    "features.createOrReplaceTempView('features_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        nvl(userId,0) as userId,\n",
    "        nvl(lengthAsFree,0) as lengthAsFree,\n",
    "        nvl(lengthAsPaid,0) as lenghtAsPaid,\n",
    "        nvl(n_help,0) as n_help,\n",
    "        nvl(n_addFriend,0) as n_addFriend,\n",
    "        nvl(usageTime,0) as usageTime\n",
    "    FROM\n",
    "        features_tbl\n",
    "    \"\"\")\n",
    "\n",
    "features.createOrReplaceTempView('features_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we include also the target label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        distinct userId,\n",
    "        churn as label\n",
    "    FROM\n",
    "        data_tbl\"\"\"\n",
    "         )\n",
    "\n",
    "targets.createOrReplaceTempView('targets_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        f.*,\n",
    "        CAST(t.label AS INT) as label\n",
    "    FROM\n",
    "        features_tbl as f,\n",
    "        targets_tbl as t\n",
    "    WHERE\n",
    "        f.userId=t.userId\n",
    "        \"\"\")\n",
    "\n",
    "features.createOrReplaceTempView('features_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Machine Learning to Predict Churn\n",
    "In this section we will train a binary classifier to predict churn. Different models (cf. below) will be used.\n",
    "In a first step, we need to vectorize the features and scale them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"lengthAsFree\", \"lenghtAsPaid\", \"n_help\", \"n_addFriend\", \"usageTime\"], outputCol=\"AllFeatures\")\n",
    "scaler = Normalizer(inputCol=\"AllFeatures\", outputCol=\"ScaledAllFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialite two different models and wrap them in a pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(featuresCol=\"ScaledAllFeatures\", labelCol=\"label\", maxIter=20, regParam=0.01)\n",
    "model_gbt = GBTClassifier(featuresCol=\"ScaledAllFeatures\", labelCol=\"label\")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, model_lr])\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, model_gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 80% of the data for training and 20% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training, test = features.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fit method of the pipeline object, we can train the models on the training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lr_fitted = pipeline_lr.fit(training)\n",
    "model_gbt_fitted = pipeline_gbt.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order the evaluate the performance of the different models, we define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_performance(model, test_data, metric = 'f1'):\n",
    "    \"\"\" Evaluate a machine learning model's performance \n",
    "    \n",
    "        Input: \n",
    "            model(object) - trained model or pipeline object\n",
    "            metric - the metric used to measure performance\n",
    "            data - test data on which performance measurement should be performed\n",
    "        Output:\n",
    "            score\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(metricName = metric)\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # calcualte score\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6238720262510253"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(model_lr_fitted, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774823708785972"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance(model_gbt_fitted, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the Logistic Regression Model performs best in terms of the metric f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
